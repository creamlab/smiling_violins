Companion code and stimuli for the work of Bedoya et al. (2021) Even violins can cry: Specifically-vocal emotional behaviours also drive the perception of emotions in non-vocal music.
 (Daniel Bedoya, Pablo Arias, Louise Goupil, Marco Liuni, Clement Canonne &amp; JJ Aucouturier, IRCAM, Paris &amp; FEMTO-ST, Besançon) 

<h2> Stimuli: </h2> 

We selected 14 excerpts from songs of various popular music genres (pop, jazz, rock), available as unmixed, multi-track recordings from the free online resource "Mixing Secrets For the Small Studio". For each recording, we selected one full musical phrase (singing + accompaniment) of average duration M = 7 sec.

For each excerpt, we then used the available multi-tracks to create variants in 4 conditions: singing (the lead vocal track, without instrumental accompaniment), singing + accompaniment (the original song, composed of lead vocal track and instrumental accompaniment), violin + accompaniment (the original song in which the lead vocal track was replaced by a violin instrumental track matching the main melody) and speech (a recording of a transcription of the lyrics of the lead vocal track, performed as non-musical speech). None of the accompaniment tracks in conditions "singing + accompaniment" and "violin + accompaniment" contained additional background vocals.

The instrumental track in the "violin + accompaniment" condition was recorded on the violin by a semi-professional musician (Choeurs et Orchestres des Grandes Écoles) in overdubbing conditions matching the pitch and phrasing of the original vocal track. Speech tracks in the "speech" condition were recorded by two native English speakers (one male, one female, matching the gender of the original singer), who performed a spoken, neutral-tone rendition of the lyrics, without knowing nor hearing that these were originally singing material. All recordings were performed in music production studios in IRCAM (Paris, France) by a professional sound engineer (D.B.). In addition, we also selected 12 "scream" stimuli from a previous study (Liuni et al. Behavioral Processes, 2020), which consisted of short, isolated shouts of phoneme /a/, recorded by 6 male and 6 female actors. These resulted in 68 sets of multi-track stimuli, matched in 5 different conditions (Speech: 14; singing: 14; singing + accompaniment: 14; violin + accompaniment: 14; and an unmatched set of 12 screams).

Before mixing, the lead track (vocal in conditions "speech", "screams", "singing", "singing + accompaniment"; violin in condition "violin + accompaniment") in each of the multi-track stimuli was then processed with three acoustic manipulations simulating specifically-vocal behaviours: smiling (two levels: smile and unsmile), vocal tremor (one level: tremor) and vocal roughness (one level: tension). Finally, the tracks of each stimulus were mixed by a professional sound engineer (DB), resulting in 68 non-manipulated and 272 manipulated stereo stimuli. 
